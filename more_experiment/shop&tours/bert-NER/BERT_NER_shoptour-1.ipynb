{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu==2.0.0b0\n",
      "  Using cached https://files.pythonhosted.org/packages/e8/7e/87c4c94686cda7066f52cbca4c344248516490acdd6b258ec6b8a805d956/tensorflow_gpu-2.0.0b0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib64/python3.6/site-packages (from tensorflow-gpu==2.0.0b0) (1.16.4)\n",
      "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 in /home/kuadmin01/.local/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0b0) (1.14.0.dev2019060501)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0b0) (0.2.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0b0) (0.8.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib64/python3.6/site-packages (from tensorflow-gpu==2.0.0b0) (1.22.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0b0) (0.33.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib64/python3.6/site-packages (from tensorflow-gpu==2.0.0b0) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib64/python3.6/site-packages (from tensorflow-gpu==2.0.0b0) (3.9.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0b0) (0.1.7)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib64/python3.6/site-packages (from tensorflow-gpu==2.0.0b0) (1.0.8)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0b0) (1.1.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0b0) (0.7.1)\n",
      "Requirement already satisfied: tb-nightly<1.14.0a20190604,>=1.14.0a20190603 in /home/kuadmin01/.local/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0b0) (1.14.0a20190603)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib64/python3.6/site-packages (from tensorflow-gpu==2.0.0b0) (1.11.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0b0) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /home/kuadmin01/.local/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0b0) (41.0.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib64/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0b0) (2.9.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib64/python3.6/site-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0b0) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib64/python3.6/site-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0b0) (0.15.5)\n",
      "Installing collected packages: tensorflow-gpu\n",
      "  Found existing installation: tensorflow-gpu 1.14.0\n",
      "    Uninstalling tensorflow-gpu-1.14.0:\n",
      "      Successfully uninstalled tensorflow-gpu-1.14.0\n",
      "\u001b[33m  WARNING: The scripts freeze_graph, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/home/kuadmin01/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed tensorflow-gpu-2.0.0b0\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 19.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-gpu==2.0.0b0 --user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: np_utils in /home/kuadmin01/.local/lib/python3.6/site-packages (0.5.10.0)\n",
      "Requirement already satisfied: numpy>=1.0 in /usr/local/lib64/python3.6/site-packages (from np_utils) (1.16.4)\n",
      "Requirement already satisfied: future>=0.16 in /home/kuadmin01/.local/lib/python3.6/site-packages (from np_utils) (0.17.1)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 19.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install np_utils --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras==2.2.4 in /home/kuadmin01/.local/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/site-packages (from keras==2.2.4) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib64/python3.6/site-packages (from keras==2.2.4) (1.16.4)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/kuadmin01/.local/lib/python3.6/site-packages (from keras==2.2.4) (1.3.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib64/python3.6/site-packages (from keras==2.2.4) (2.9.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib64/python3.6/site-packages (from keras==2.2.4) (1.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib64/python3.6/site-packages (from keras==2.2.4) (1.0.8)\n",
      "Requirement already satisfied: pyyaml in /home/kuadmin01/.local/lib/python3.6/site-packages (from keras==2.2.4) (5.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 19.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==2.2.4 --user\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /home/kuadmin01/.local/lib/python3.6/site-packages (3.8.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib64/python3.6/site-packages (from gensim) (1.16.4)\n",
      "Requirement already satisfied: smart-open>=1.7.0 in /home/kuadmin01/.local/lib/python3.6/site-packages (from gensim) (1.8.4)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/kuadmin01/.local/lib/python3.6/site-packages (from gensim) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: requests in /home/kuadmin01/.local/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim) (2.22.0)\n",
      "Requirement already satisfied: boto3 in /home/kuadmin01/.local/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim) (1.9.198)\n",
      "Requirement already satisfied: boto>=2.32 in /home/kuadmin01/.local/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/kuadmin01/.local/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kuadmin01/.local/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim) (2019.6.16)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/kuadmin01/.local/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/kuadmin01/.local/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim) (1.25.3)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /home/kuadmin01/.local/lib/python3.6/site-packages (from boto3->smart-open>=1.7.0->gensim) (0.2.1)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.198 in /home/kuadmin01/.local/lib/python3.6/site-packages (from boto3->smart-open>=1.7.0->gensim) (1.12.198)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/kuadmin01/.local/lib/python3.6/site-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.4)\n",
      "Requirement already satisfied: docutils<0.15,>=0.10 in /home/kuadmin01/.local/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.198->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /home/kuadmin01/.local/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.198->boto3->smart-open>=1.7.0->gensim) (2.8.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 19.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras==2.2.4 in /home/kuadmin01/.local/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib64/python3.6/site-packages (from keras==2.2.4) (1.16.4)\n",
      "Requirement already satisfied: h5py in /usr/local/lib64/python3.6/site-packages (from keras==2.2.4) (2.9.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib64/python3.6/site-packages (from keras==2.2.4) (1.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/site-packages (from keras==2.2.4) (1.12.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/kuadmin01/.local/lib/python3.6/site-packages (from keras==2.2.4) (1.3.0)\n",
      "Requirement already satisfied: pyyaml in /home/kuadmin01/.local/lib/python3.6/site-packages (from keras==2.2.4) (5.1.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib64/python3.6/site-packages (from keras==2.2.4) (1.0.8)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 19.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==2.2.4 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras  as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk, pos_tag\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, PunktSentenceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import gensim.models.word2vec as w2v\n",
    "from gensim import logging\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as  pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from  keras.preprocessing.text import Tokenizer\n",
    "from  keras.preprocessing.sequence import pad_sequences\n",
    "import json\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/kuadmin01/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['view', 'views', 'sunset', 'seaside', 'sea']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_files = ['naturetravel.txt']\n",
    "word_dishes = []\n",
    "for word_file in word_files:\n",
    "    with open(word_file) as f:\n",
    "        lines = f.readlines()\n",
    "    word_list2 = [x.strip() for x in lines] \n",
    "    \n",
    "    word_dishes.extend(word_list2)\n",
    "word_dishes[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['street', 'markets', 'street food', 'streetfood', 'shop']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_files = ['shoppingtravel.txt']\n",
    "word_restaurant = []\n",
    "for word_file in word_files:\n",
    "    with open(word_file) as f:\n",
    "        lines = f.readlines()\n",
    "    word_list2 = [x.strip() for x in lines] \n",
    "    \n",
    "    word_restaurant.extend(word_list2)\n",
    "word_restaurant[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_files = ['shopping&market.txt']\n",
    "#word_location = []\n",
    "#for word_file in word_files:\n",
    "#    with open(word_file) as f:\n",
    "#        lines = f.readlines()\n",
    "#    lists_l= [x.strip() for x in lines] \n",
    "    \n",
    "#    word_location.extend(lists_l)\n",
    "#word_location[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates\n",
    "def ordered_set(in_list):\n",
    "    out_list = []\n",
    "    added = set()\n",
    "    for val in in_list:\n",
    "        if not val in added:\n",
    "            out_list.append(val)\n",
    "            added.add(val)\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dishes = ordered_set(word_dishes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_restaurant = ordered_set(word_restaurant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_location = ordered_set(word_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('street', 'food'), ('walking', 'street'), ('food', 'court')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mw_list = []\n",
    "for x in word_dishes:\n",
    "    ws = x.split() \n",
    "    if len(ws) > 1:\n",
    "        tws = tuple(ws)\n",
    "        mw_list.append(tws)\n",
    "        \n",
    "\n",
    "for x in word_restaurant:\n",
    "    ws = x.split() \n",
    "    if len(ws) > 1:\n",
    "        tws = tuple(ws)\n",
    "        mw_list.append(tws)\n",
    "        \n",
    "\n",
    "        \n",
    "#for x in word_location:\n",
    "#    ws = x.split() \n",
    "#    if len(ws) > 1:\n",
    "#        tws = tuple(ws)\n",
    "#        mw_list.append(tws)\n",
    "        \n",
    "mw_list = ordered_set(mw_list)\n",
    "mw_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "my_idx = {}\n",
    "for i,w in enumerate(mw_list):\n",
    "    my_idx[\" \".join(w)] = i\n",
    "print(len(my_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in mw_list:\n",
    "    if i == ('friendly','service'):\n",
    "        print('found')\n",
    "    if i == ('asian','food'):\n",
    "        print('found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwe = MWETokenizer(mw_list,separator=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"BERT-ShopTours-NER.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8894, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000 bhat on a wooden standing lanna buddha whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10 mins drive and we arrived at the elephant h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10 yrs and 12 yrs did not really expect anythi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100 503920 e 13 45 12 5 n 100 30 14 1 e within...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100 recommended place they really care about t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             summary\n",
       "0  000 bhat on a wooden standing lanna buddha whi...\n",
       "1  10 mins drive and we arrived at the elephant h...\n",
       "2  10 yrs and 12 yrs did not really expect anythi...\n",
       "3  100 503920 e 13 45 12 5 n 100 30 14 1 e within...\n",
       "4  100 recommended place they really care about t..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.dropna()\n",
    "reviews = reviews.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spans(txt):\n",
    "    tokens=mwe.tokenize(word_tokenize(txt.lower()))\n",
    "    offset = 0\n",
    "    for token in tokens:\n",
    "        offset = txt.find(token, offset)\n",
    "        yield token, offset, offset+len(token)\n",
    "        offset += len(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        # We are not using \"text.split()\" here\n",
    "        #since it is not fool proof, e.g. words followed by punctuations \"Are you kidding?I think you aren't.\"\n",
    "        text = re.findall(r\"[\\w']+\", text)\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)# remove links\n",
    "    text = re.sub(r'\\<a href', ' ', text)# remove html link tag\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.!?:#$\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restaurants are complete.\n"
     ]
    }
   ],
   "source": [
    "clean_restaurant = []\n",
    "for restaurants in reviews.summary:\n",
    "    clean_restaurant.append(clean_text(restaurants, remove_stopwords=False))\n",
    "print(\"restaurants are complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "074ca890349b4b81833e4c42f642bd1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8894), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "all_item = []\n",
    "\n",
    "for i in tqdm(range(len(reviews))):\n",
    "    word_ls = []\n",
    "    #print(\"sentence:\",i+1)\n",
    "    \n",
    "    for token in spans(clean_restaurant[i]):\n",
    "        #print(token)\n",
    "        #assert token[0]==reviews.location[i][token[1]:token[2]]\n",
    "        my_tuple = token[0]\n",
    "        #print(\"###\",token)\n",
    "        \n",
    "        #my_tuples = ' , '.join(map(str, my_tuple))\n",
    "        if token[0] in word_dishes:\n",
    "            #word_ls.append(my_tuple)\n",
    "            subwords = token[0].split()\n",
    "            pos_list = [nltk.pos_tag([w]) for w in subwords]\n",
    "            tag_list = ['I-NTV']*len(pos_list)\n",
    "            tag_list[0] = 'B-NTV' \n",
    "            \n",
    "            \n",
    "            for s,p,t in zip(subwords,pos_list,tag_list):           \n",
    "                if type(p) == list:\n",
    "                    p = p[0][1]\n",
    "                    #print('list')\n",
    "                new_item = dict({'Sentence #': i+1, 'Tag' : t, 'Word': s,'POS': p})\n",
    "                all_item.append(new_item)\n",
    "                \n",
    "                \n",
    "            #lis_lo = nltk.pos_tag(word_ls),LOC\n",
    "            #print(' , '.join(map(str, lis_lo)))\n",
    " \n",
    "        elif token[0] in word_restaurant:\n",
    "            \n",
    "            subwords = token[0].split()\n",
    "            pos_list = [nltk.pos_tag([w]) for w in subwords]\n",
    "            tag_list = ['I-STV']*len(pos_list)\n",
    "            tag_list[0] = 'B-STV' \n",
    "            \n",
    "            #print(subwords)\n",
    "            for s,p,t in zip(subwords,pos_list,tag_list):           \n",
    "                new_item = dict({'Sentence #': i+1, 'Tag' : t, 'Word': s,'POS': p[0][1]})\n",
    "                all_item.append(new_item)\n",
    "                #print(new_item)\n",
    "            #word_ls.append(my_tuple)\n",
    "            #print(\"found LOC\")\n",
    "            #lis_lo = nltk.pos_tag(word_ls),\"LOC\"\n",
    "            #print(' , '.join(map(str, lis_lo)))\n",
    "             \n",
    "#        elif token[0] in word_location:\n",
    "            #word_ls.append(my_tuple)\n",
    "            #lis_lo = nltk.pos_tag(word_ls),\"FACILITY\"\n",
    "            #print(' , '.join(map(str, lis_lo)))\n",
    "            \n",
    "            #my_pos = nltk.pos_tag([my_tuple])[0][1]\n",
    "            #new_item =  dict({'Sentence #': i+1, 'Tag' : 'FACILITY', 'Word': my_tuple,'POS': my_pos})\n",
    "            \n",
    "#            subwords = token[0].split()\n",
    "#            pos_list = [nltk.pos_tag([w]) for w in subwords]\n",
    "#            tag_list = ['I-SHOP']*len(pos_list)\n",
    "#            tag_list[0] = 'B-SHOP' \n",
    "            \n",
    "             \n",
    "#            for s,p,t in zip(subwords,pos_list,tag_list):   \n",
    "#                if type(p) == list:\n",
    "#                    p = p[0][1]\n",
    "#                new_item = dict({'Sentence #': i+1, 'Tag' : t, 'Word': s,'POS': p})\n",
    "#                all_item.append(new_item)\n",
    "                \n",
    "\n",
    "             \n",
    "        else:\n",
    "            #print(type(my_tuple))\n",
    "            my_pos = nltk.pos_tag([my_tuple])[0][1]\n",
    "            new_item = dict({'Sentence #': i+1, 'Tag' : 'O', 'Word': my_tuple.lower(),'POS': my_pos})\n",
    "            all_item.append(new_item)\n",
    "            \n",
    "            #if i+1 == 5177:\n",
    "                #print(new_item)\n",
    "             \n",
    "            #print(nltk.pos_tag([my_tuple]),',','O')\n",
    "        #print(new_item)\n",
    "        #if not(my_pos == '.' or  my_pos == ',' or my_pos == ':' or my_pos == '(' or my_pos == ')') :\n",
    "            \n",
    "            #all_item.append(new_item) \n",
    "\n",
    "file_csv = 'ShopTour-bert.csv'\n",
    "with open(file_csv, 'w') as csv_file:\n",
    "    csv_file.write('Sentence #*Word*POS*Tag\\n')\n",
    "    for item in all_item:      \n",
    "        #print(item['POS'])    \n",
    "        csv_file.write(str(item['Sentence #'])+'*'+item['Word']+'*'+item['POS']+'*'+item['Tag']+'\\n')\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>000</td>\n",
       "      <td>CD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>bhat</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>on</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>wooden</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>standing</td>\n",
       "      <td>VBG</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>lanna</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>buddha</td>\n",
       "      <td>NN</td>\n",
       "      <td>B-NTV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>which</td>\n",
       "      <td>WDT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence #      Word  POS    Tag\n",
       "0           1       000   CD      O\n",
       "1           1      bhat   IN      O\n",
       "2           1        on   IN      O\n",
       "3           1         a   DT      O\n",
       "4           1    wooden   NN      O\n",
       "5           1  standing  VBG      O\n",
       "6           1     lanna   NN      O\n",
       "7           1    buddha   NN  B-NTV\n",
       "8           1     which  WDT      O\n",
       "9           1       the   DT      O"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "#load data\n",
    "# explore data\n",
    "file_csv = 'ShopTour-bert.csv'\n",
    "data = pd.read_csv(file_csv, sep='*',encoding=\"utf-8\").fillna(method=\"ffill\")\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat sentence\n",
    "getter = SentenceGetter(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = [ [s[0] for s in sent] for sent in getter.sentences] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000',\n",
       " 'bhat',\n",
       " 'on',\n",
       " 'a',\n",
       " 'wooden',\n",
       " 'standing',\n",
       " 'lanna',\n",
       " 'buddha',\n",
       " 'which',\n",
       " 'the',\n",
       " 'owner',\n",
       " 'actually']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = word_list\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8894\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NTV', 'O', 'O', 'O', 'O']\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "labels = [[s[2] for s in sent] for sent in getter.sentences]\n",
    "print(labels[0])\n",
    "print(len(labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(word_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8894\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_vals = list(set(data[\"Tag\"].values))\n",
    "tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
    "idx2tag = {i: t for i, t in enumerate(tags_vals) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11413"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = list(set(data[\"Word\"].values))\n",
    "n_words = len(words); \n",
    "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "word2idx[\"UNK\"] = 1\n",
    "word2idx[\"PAD\"] = 0\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B-STV'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2tag[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('pytorch-pretrained-BERT/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "bs = 32\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "n_gpu = torch.cuda.device_count()\n",
    "#torch.cuda.get_device_name(0)\n",
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', 'bhat', 'on', 'a', 'wooden', 'standing', 'lanna', 'buddha', 'which', 'the', 'owner', 'actually']\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = word_list\n",
    "print(tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000',\n",
       " 'bhat',\n",
       " 'on',\n",
       " 'a',\n",
       " 'wooden',\n",
       " 'standing',\n",
       " 'lanna',\n",
       " 'buddha',\n",
       " 'which',\n",
       " 'the',\n",
       " 'owner',\n",
       " 'actually']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 12\n"
     ]
    }
   ],
   "source": [
    "tokens_ids = [[word2idx[w] for w in s] for s in tokenized_texts]\n",
    "print(len(tokens_ids[0]),len(labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 12\n"
     ]
    }
   ],
   "source": [
    " print(len(tokens_ids[0]),len(labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "input_ids = pad_sequences(tokens_ids,\n",
    "                          maxlen=MAX_LEN, dtype=\"int64\", truncating=\"post\", padding=\"post\")\n",
    "#input_ids = pad_sequences([convert_word2idx(txt) for txt in tokenized_texts],\n",
    "                          #maxlen=MAX_LEN, dtype=\"int64\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "print(MAX_LEN)\n",
    "for i in tokens_ids:\n",
    "    if len(i) > MAX_LEN:\n",
    "         \n",
    "        i = i[:128]\n",
    "        print(\"need more\")\n",
    "         \n",
    "        #MAX_LEN = len(i)\n",
    "print(MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_list = [[tag2idx.get(l) for l in lab] for lab in labels]\n",
    "len(t_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4676  3385  5620  6739  7279 10233 10035  4846 10862  8807 10300  8138\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n",
    "                     dtype=\"int64\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train test\n",
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change to torhc tensor\n",
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8004, 128])\n",
      "torch.Size([8004, 128])\n",
      "torch.Size([8004, 128])\n"
     ]
    }
   ],
   "source": [
    "print(tr_inputs.shape)\n",
    "print(tr_masks.shape)\n",
    "print(tr_tags.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([890, 128])\n",
      "torch.Size([890, 128])\n",
      "torch.Size([890, 128])\n"
     ]
    }
   ],
   "source": [
    "print(val_inputs.shape)\n",
    "print(val_masks.shape)\n",
    "print(val_tags.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(u\"bert-large-uncased\", num_labels=len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine tune BERT\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.13749892727854066\n",
      "Validation loss: 0.03210216985150639\n",
      "Validation Accuracy: 0.9861617874313187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|         | 1/10 [03:40<33:08, 220.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.6075638506876228\n",
      "Train loss: 0.015150981430549628\n",
      "Validation loss: 0.010620615900864192\n",
      "Validation Accuracy: 0.9953680352850275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|        | 2/10 [07:21<29:27, 220.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.8419083255378859\n",
      "Train loss: 0.006655683868056334\n",
      "Validation loss: 0.007395145330519881\n",
      "Validation Accuracy: 0.9978932810353708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|       | 3/10 [11:02<25:46, 220.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.9149223497636731\n",
      "Train loss: 0.005148233387009816\n",
      "Validation loss: 0.006112073699374118\n",
      "Validation Accuracy: 0.9981045565762363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|      | 4/10 [14:43<22:05, 220.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.9261517615176151\n",
      "Train loss: 0.006690079184720734\n",
      "Validation loss: 0.006044416599512\n",
      "Validation Accuracy: 0.9988249055631868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|     | 5/10 [18:24<18:24, 220.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.9551506657323054\n",
      "Train loss: 0.003535031340001717\n",
      "Validation loss: 0.008191175177541612\n",
      "Validation Accuracy: 0.9992655659769918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|    | 6/10 [22:05<14:43, 220.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.9665480427046264\n",
      "Train loss: 0.002662287327718125\n",
      "Validation loss: 0.007190318631355045\n",
      "Validation Accuracy: 0.9979985834478021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  70%|   | 7/10 [25:45<11:02, 220.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.9221435793731042\n",
      "Train loss: 0.00366818371854838\n",
      "Validation loss: 0.004774849034479952\n",
      "Validation Accuracy: 0.9997712858430632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|  | 8/10 [29:26<07:21, 220.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.9899062725306416\n",
      "Train loss: 0.0029245113663243076\n",
      "Validation loss: 0.0032410668554803124\n",
      "Validation Accuracy: 0.9997471400669643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  90%| | 9/10 [33:07<03:40, 220.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.988555078683834\n",
      "Train loss: 0.0009714847630233584\n",
      "Validation loss: 0.0029831355813030314\n",
      "Validation Accuracy: 0.9986397879464286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 100%|| 10/10 [36:47<00:00, 220.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.946103673189152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # TRAIN loop\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # forward pass\n",
    "        loss = model(b_input_ids, token_type_ids=None,\n",
    "                     attention_mask=b_input_mask, labels=b_labels)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    # VALIDATION on validation set\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels, true_inputs = [], [],[]\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "                                  attention_mask=b_input_mask, labels=b_labels)\n",
    "            logits = model(b_input_ids, token_type_ids=None,\n",
    "                           attention_mask=b_input_mask)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        inputs = b_input_ids.to('cpu').numpy()\n",
    "         \n",
    "        true_inputs.append(inputs)\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.append(label_ids)\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        \n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "    eval_loss = eval_loss/nb_eval_steps\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "    pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n",
    "    valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
    "    valid_inputs = [[idx2word[l_ii] for l_ii in l_i] for l in  true_inputs  for l_i in l ]\n",
    "\n",
    "\n",
    "    print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
